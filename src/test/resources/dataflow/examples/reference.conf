


spark {
     spark.executor.memory=1g
     spark.executor.cores=1
     spark.executor.instances=1

}

source {

 kafka {
    kafka.bootstrap.servers="registry:9092"
    topics="test"
    result_table_name="test"
    consumer.failOnDataLoss=false
 }


file {
            path = "s3a://test/people.json"
            format="json"
            result_table_name="json"
    }

file2 {
            path = "s3a://test/people.parquet"
            format="parquet"
            result_table_name="parquet"
    }

 jdbc {
        tag=meta
        numPartitions=4
        partitionColumn=HASH_CODE
        lowerBound=0
        upperBound=4
        table="(select MOD(partitionColumnName,4) AS HASH_CODE, T.* FROM utol.cloud_load_log as t) as tx"
        result_table_name=cll
    }

}

processor {

     sql {
        sql = "select * ,current_date() as dt from json limit "
        result_table_name="test"
     }

     trim2 {
            table="json"
            result_table_name="json2"
        }

     sql3 {
            sql="select * from (select *, row_number() over(partition by SAP_SALES_ORD_NUM,LINE_ITEM_SEQ_NUM order by A_TIMSTAMP DESC, A_CCID DESC) as rowno from json2) as t where rowno = 1 and A_ENTTYP <> 'DL'"
            result_table_name="json3"
      }

     drop4{
              table="json3"
              source_field=[rowno]
              result_table_name="json4"
          }

      rename5 {
            table="json4"
            fields =[
                 {
                     field=col_old
                     new_field=col_new
                 }
                 {
                  field=col_old2
                  new_field=col_new2
                  }
             ]
            result_table_name="json5"
      }

      replace6  {
            table="json5"
            fields =[
                 {
                 source_field="col_old"
                 target_field="col_new"
                 pattern="[^\p{ASCII}]$"
                 replacement=""
                 }
              ]
            result_table_name="json6"
      }

      sql7 {
              inferSource=true #not need write source {} for this sql
              sqlfile= "Load_Allocation_Fact"
              result_table_name= "ALLOCATION_FACT_VIEW"
          }

}


sink {

    dashdb {
        inNewView= "json6"
        batchSize= 10000
        numPartitions= 10
        strategy= "Full" #incremental
        result_table_name= "test.sales_order_line_item"
        priKeys = [
           {key=IMAGE_PART}
        ]
    }

    jdbc {
           tag="meta"
           table=""  # this is the view name in spark catalog
           result_table_name="" #this is which table write to db
           batchSize=100
           savemode=append
    }

    stdout {
        table="json6"
        limit=20
        format="json" #plain
    }

    file {
       table="parquet"
       path="s3a://test/people/"
       savemode="overwrite"
       options.parquet.compression="gzip"
    }


    newrelic1{
              tag=meta
              table=newrelic
              eventType =recon_cdc_drop_test
        }

}
