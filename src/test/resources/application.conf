
classmap {
    source.set=org.student.spark.source.SetVar
    source.rate=org.student.spark.source.RateStream
    source.static=org.student.spark.source.StringData
    source.kafka=org.student.spark.source.Kafka
    source.dummy=org.student.spark.source.Dummy
    source.jdbc=org.student.spark.source.Jdbc
    source.file=org.student.spark.source.File
    sink.cos=org.student.spark.sink.COSFile
    sink.jdbc=org.student.spark.sink.Jdbc
    sink.stdout=org.student.spark.sink.Console
    sink.file=org.student.spark.sink.File
    sink.postgre=org.student.spark.sink.Postgresql
    sink.email=org.student.spark.sink.Email
    sink.kafka=org.student.spark.sink.Kafka
    sink.delta=org.student.spark.sink.Delta
    sink.db=org.student.spark.sink.DB
    processor.sql=org.student.spark.processor.Sql
    processor.drop=org.student.spark.processor.Drop
    processor.rename=org.student.spark.processor.Rename
    processor.repartition=org.student.spark.processor.Repartition
    processor.replace=org.student.spark.processor.Replace
    processor.trim=org.student.spark.processor.Trim
}

spark {
        #change default compression snappy to gzip for use less storage
      spark.sql.parquet.compression.codec=gzip
      #write deltatable data to s3
      spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore
      #spark.hadoop.hive.metastore.uris="thrift://localhost:9083"
      #spark.delta.logStore.class=org.student.spark.common.S3LogStore
      spark.sql.extensions="io.delta.sql.DeltaSparkSessionExtension"
      spark.sql.catalog.spark_catalog="org.apache.spark.sql.delta.catalog.DeltaCatalog"
      spark.sql.catalogImplementation=hive
      spark.scheduler.mode=FAIR
      spark.port.maxRetries=100
      spark.test.useEmbedComponents=false
}

app {
    debug=true
    sql.dir=/tmp/sql
    show.sample=false
    zone.id="UTC" # todo

    web {
      io.threads=2
      worker.threads=10
      server.port=8080
      ui.path=/home/spark/mnt/web/ui/
    }

    k8s {
        secrets.enabled=false
        secrets.file=datatrans.properties
        secrets.dir =/tmp/k8s/secrets/vt/
    }


    jdbc {
        connection.timeout = 120 #unit is seconds
        pool.maxsize = 30
    }

}


app.jdbc.test.url="jdbc:postgresql:postgres"
app.jdbc.test.username=postgres
app.jdbc.test.driver=org.postgresql.Driver
app.jdbc.test.password=postgres

app.jdbc.meta.url="jdbc:postgresql:postgres"
app.jdbc.meta.username=postgres
app.jdbc.meta.driver=org.postgresql.Driver
app.jdbc.meta.password=postgres

spark.hadoop.fs.cos.service.secret.key=test
spark.hadoop.fs.cos.service.access.key=test
spark.hadoop.fs.cos.service.endpoint="localhost:9091"
spark.hadoop.fs.cos.connection.ssl.enabled=false

#spark.hadoop.fs.cos.service.secret.key=minio123
#spark.hadoop.fs.cos.service.access.key=minio
#spark.hadoop.fs.cos.service.endpoint="localhost:9000"
#spark.hadoop.fs.cos.connection.ssl.enabled=false
