
classmap {
    source.set=org.student.spark.source.SetVar
    source.rate=org.student.spark.source.RateStream
    source.static=org.student.spark.source.StringData
    source.kafka=org.student.spark.source.Kafka
    source.dummy=org.student.spark.source.Dummy
    source.jdbc=org.student.spark.source.Jdbc
    source.file=org.student.spark.source.File
    sink.jdbc=org.student.spark.sink.Jdbc
    sink.stdout=org.student.spark.sink.Console
    sink.file=org.student.spark.sink.File
    sink.postgre=org.student.spark.sink.Postgresql
    sink.email=org.student.spark.sink.Email
    sink.delta=org.student.spark.sink.Delta
    sink.kafka=org.student.spark.sink.Kafka
    processor.sql=org.student.spark.processor.Sql
    processor.drop=org.student.spark.processor.Drop
    processor.rename=org.student.spark.processor.Rename
    processor.repartition=org.student.spark.processor.Repartition
    processor.replace=org.student.spark.processor.Replace
    processor.trim=org.student.spark.processor.Trim
}

spark {
              ###the following is used for cos
              spark.hadoop.fs.stocator.scheme.list=cos
              spark.hadoop.fs.cos.impl=com.ibm.stocator.fs.ObjectStoreFileSystem
              spark.hadoop.fs.stocator.cos.impl=com.ibm.stocator.fs.cos.COSAPIClient
              spark.hadoop.fs.stocator.cos.scheme=cos
              spark.hadoop.fs.cos.client.execution.timeout=18000000
              spark.hadoop.fs.cos.service.v2.signer.type=false
              ##for timeout issue config
             # spark.hadoop.fs.cos.connection.maximum=20000
             # spark.hadoop.fs.cos.fast.upload=true
             # spark.hadoop.fs.cos.fast.upload.buffer=bytebuffer
             # spark.hadoop.fs.cos.multipart.size=104857600
             # spark.hadoop.fs.cos.fast.upload.active.blocks=8

              #change default compression snappy to gzip for use less storage
             spark.sql.parquet.compression.codec=gzip

             #spark.hadoop.hive.metastore.uris="thrift://localhost:9083"
             #spark.sql.extensions="io.delta.sql.DeltaSparkSessionExtension"
             #spark.sql.catalog.spark_catalog="org.apache.spark.sql.delta.catalog.DeltaCatalog"
             #spark.sql.catalogImplementation=hive
}

app {
    debug=false
    show.sample=false
    zone.id="UTC" # todo
    sql.dir=/tmp/sdf/sql/


    web {
      io.threads=2
      worker.threads=10
      server.port=8080
      ui.path=/home/spark/mnt/web/ui/
    }

    k8s {
        secrets.enabled=true
        secrets.file=datatrans.properties
        secrets.dir =/opt/spark/secrets/
    }


    jdbc {
        connection.timeout = 120 #unit is seconds
        pool.maxsize = 30
    }

}